{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "6d312e52-2abe-4a98-b8fc-54929371a3af",
      "metadata": {},
      "source": [
        "# Generating STAC Machine Learning Model (MLM) Items from a YAML Config Folder\n",
        "\n",
        "This notebook reads all YAML configuration files in the [`configs/downscaling`](../configs/downscaling) folder\n",
        "and creates a STAC Item using the [MLM Extension](https://github.com/stac-extensions/mlm).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7c3da74-5c5f-463f-87b1-8886a47b3125",
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pystac stac-model==0.4.0 resoterre"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eedb58d6-0db0-427a-99df-56e515a55487",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import OrderedDict\n",
        "from dataclasses import asdict\n",
        "from pathlib import Path\n",
        "\n",
        "import pystac\n",
        "import shapely\n",
        "import yaml\n",
        "from dateutil.parser import parse as parse_dt\n",
        "from pystac import STACValidationError\n",
        "from pystac.extensions.datacube import DatacubeExtension, Dimension, Variable\n",
        "from stac_model.input import InputStructure, ModelInput\n",
        "from stac_model.output import ModelOutput, ModelResult\n",
        "from stac_model.schema import MLModelExtension, MLModelProperties\n",
        "\n",
        "from resoterre.datasets.hrdps.hrdps_variables import hrdps_variables\n",
        "from resoterre.datasets.rdps.rdps_variables import rdps_variables\n",
        "from resoterre.ml.network_manager import nb_of_parameters\n",
        "from resoterre.ml.neural_networks_unet import UNet, UNetConfig"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d39915f-22d1-4c39-8562-bfbcd2d8d9a9",
      "metadata": {},
      "source": [
        "## Utils function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1489801-604c-49bb-a7b9-9c82a1d844fa",
      "metadata": {},
      "outputs": [],
      "source": [
        "def config_aggregator(config_folder: str | Path) -> dict:\n",
        "    \"\"\"\n",
        "    Aggregate all YAML configuration files in a folder into a single dictionary.\n",
        "\n",
        "    Args:\n",
        "        config_folder (str | Path): Path to the folder containing YAML config files.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        dict: Aggregated configuration from all YAML files.\n",
        "    \"\"\"\n",
        "    config_folder = Path(config_folder)\n",
        "    if not config_folder.exists():\n",
        "        raise FileNotFoundError(f\"Config folder not found: {config_folder}\")\n",
        "\n",
        "    aggregated_cfg = {}\n",
        "\n",
        "    for yaml_file in sorted(config_folder.glob(\"*.yaml\")):\n",
        "        with yaml_file.open(\"r\", encoding=\"utf-8\") as f:\n",
        "            cfg = yaml.safe_load(f) or {}  # treat empty YAML as empty dict\n",
        "            aggregated_cfg.update(cfg)\n",
        "\n",
        "    return aggregated_cfg\n",
        "\n",
        "\n",
        "def reorder_stac_json(json_path: str | Path, top_keys: list[str]):\n",
        "    \"\"\"Reorder keys in a saved STAC JSON file so that top_keys appear right after 'id'.\"\"\"\n",
        "    json_path = Path(json_path)\n",
        "    with json_path.open() as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    ordered = OrderedDict()\n",
        "    for key in [\"type\", \"stac_version\", \"stac_extensions\", \"id\"]:\n",
        "        if key in data:\n",
        "            ordered[key] = data.pop(key)\n",
        "    for key in top_keys:\n",
        "        if key in data:\n",
        "            ordered[key] = data.pop(key)\n",
        "    for k, v in data.items():\n",
        "        ordered[k] = v\n",
        "\n",
        "    with json_path.open(\"w\") as f:\n",
        "        json.dump(ordered, f, indent=2)\n",
        "\n",
        "\n",
        "def get_variable_dimensions(var_name: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    Get datacube dimensions for a variable using the VariableHandler collections.\n",
        "\n",
        "    Variables with level dimension:\n",
        "    - Have vertical_level attribute set (e.g., TT850, HU700)\n",
        "    - Name contains 'model_levels' or 'pressure_levels' (spans multiple levels)\n",
        "    - Name ends with numeric level suffix (e.g., HRDPS_P_TT_10000)\n",
        "    \"\"\"\n",
        "    base_dims = [\"time\", \"latitude\", \"longitude\"]\n",
        "\n",
        "    # Check for multi-level naming patterns first (works for any variable)\n",
        "    if \"model_levels\" in var_name or \"pressure_levels\" in var_name:\n",
        "        return base_dims + [\"level\"]\n",
        "\n",
        "    # Try to find the variable in rdps_variables or hrdps_variables\n",
        "    var_handler = None\n",
        "    if var_name in rdps_variables:\n",
        "        var_handler = rdps_variables[var_name]\n",
        "    elif var_name in hrdps_variables:\n",
        "        var_handler = hrdps_variables[var_name]\n",
        "\n",
        "    # If variable has vertical_level set, it has a level dimension\n",
        "    if var_handler is not None and var_handler.vertical_level is not None:\n",
        "        return base_dims + [\"level\"]\n",
        "\n",
        "    # For variables not in handlers, check for numeric level suffix (e.g., HRDPS_P_TT_10000)\n",
        "    if var_handler is None:\n",
        "        parts = var_name.split(\"_\")\n",
        "        last_part = parts[-1]\n",
        "        if last_part.isdigit() and len(last_part) >= 3:\n",
        "            return base_dims + [\"level\"]\n",
        "\n",
        "    return base_dims"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a25bb219-a3fe-4c15-83f3-dfdffafb3ec6",
      "metadata": {},
      "source": [
        "## Function to convert a config to STAC item\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "160019e2-cd05-431d-beb1-1201903443c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_stac_item_from_config(yaml_config: dict):\n",
        "    \"\"\"\n",
        "    Create a STAC Item (and associated Collection) from a YAML model configuration dicttionary.\n",
        "\n",
        "    Args:\n",
        "        yaml_config (dict): Aggregated YAML configuration dictionary describing the model.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "        pystac.Item: The generated STAC Item.\n",
        "    \"\"\"\n",
        "    cfg = yaml_config\n",
        "\n",
        "    train_cfg = cfg[\"train_dataset\"]\n",
        "    model_path = cfg[\"path_models\"] + \"/unet-model.ckpt\"\n",
        "    model_name = \"UNet\"\n",
        "    in_channels = cfg[\"networks\"][\"UNet\"][\"in_channels\"]\n",
        "    out_channels = cfg[\"networks\"][\"UNet\"][\"out_channels\"]\n",
        "    depth = cfg[\"networks\"][\"UNet\"][\"depth\"]\n",
        "    initial_nb_of_hidden_channels = cfg[\"networks\"][\"UNet\"][\"initial_nb_of_hidden_channels\"]\n",
        "\n",
        "    # --- Model architecture and parameters ---\n",
        "    config = UNetConfig(\n",
        "        in_channels=in_channels,\n",
        "        out_channels=out_channels,\n",
        "        depth=depth,\n",
        "        initial_nb_of_hidden_channels=initial_nb_of_hidden_channels,\n",
        "    )\n",
        "    model = UNet(**asdict(config))\n",
        "    nb_params = nb_of_parameters(model)\n",
        "    module = model.__class__.__module__\n",
        "    class_name = model.__class__.__name__\n",
        "    arch = f\"{module}.{class_name}\"\n",
        "    framework = cfg[\"framework\"]\n",
        "    framework_version = cfg[\"framework_version\"]\n",
        "\n",
        "    # --- Input/Output shapes ---\n",
        "    input_variables = train_cfg.get(\"rdps_variables\", [])\n",
        "    output_variables = train_cfg.get(\"hrdps_variables\", [])\n",
        "    height, width = 64, 64  # TODO: Add to config\n",
        "\n",
        "    # --- Input structure ---\n",
        "    input_struct = InputStructure(\n",
        "        shape=[-1, len(input_variables), height, width],\n",
        "        dim_order=[\"time\", \"variables\", \"latitude\", \"longitude\"],\n",
        "        data_type=\"float32\",\n",
        "    )\n",
        "\n",
        "    # --- Processing expression for pre-processing function ---\n",
        "    preprocessing_command = cfg.get(\"preprocessing_command\", None)\n",
        "    if preprocessing_command:\n",
        "        # Remove line continuations (backslash + newline) and collapse to single line\n",
        "        clean_command = preprocessing_command.replace(\"\\\\\\n\", \" \").replace(\"\\n\", \" \")\n",
        "        # Collapse multiple spaces into one\n",
        "        clean_command = \" \".join(clean_command.split())\n",
        "        pre_processing_function = {\n",
        "            \"format\": \"snakemake\",\n",
        "            \"expression\": clean_command,\n",
        "        }\n",
        "\n",
        "    model_input = ModelInput(\n",
        "        name=\"rdps_inputs\",\n",
        "        variables=input_variables,\n",
        "        input=input_struct,\n",
        "        pre_processing_function=pre_processing_function,\n",
        "    )\n",
        "\n",
        "    # --- Output structure ---\n",
        "    result_struct = ModelResult(\n",
        "        shape=[-1, len(output_variables), height, width],\n",
        "        dim_order=[\"time\", \"variables\", \"latitude\", \"longitude\"],\n",
        "        data_type=\"float32\",\n",
        "    )\n",
        "\n",
        "    model_output = ModelOutput(\n",
        "        name=\"hrdps_outputs\",\n",
        "        variables=output_variables,\n",
        "        tasks=[\"super-resolution\", \"downscaling\"],\n",
        "        result=result_struct,\n",
        "    )\n",
        "\n",
        "    # --- ML Model Properties ---\n",
        "    ml_model_meta = MLModelProperties(\n",
        "        name=f\"{model_name} RDPS HRDPS Downscaling\",\n",
        "        architecture=arch,\n",
        "        tasks={\"super-resolution\", \"downscaling\"},\n",
        "        framework=framework,\n",
        "        framework_version=framework_version,\n",
        "        accelerator=cfg.get(\"device\", \"cuda\"),\n",
        "        pretrained=True,\n",
        "        pretrained_source=\"Custom RDPS-HRDPS dataset\",\n",
        "        input=[model_input],\n",
        "        output=[model_output],\n",
        "        total_parameters=nb_params,\n",
        "    )\n",
        "\n",
        "    # --- Assets ---\n",
        "    assets = {\n",
        "        \"model\": pystac.Asset(\n",
        "            title=f\"{model_name} checkpoint\",\n",
        "            description=f\"{model_name} trained on RDPS HRDPS downscaling task.\",\n",
        "            href=model_path,\n",
        "            media_type=\"application/octet-stream; application/pytorch\",\n",
        "            roles=[\"mlm:model\", \"mlm:weights\"],\n",
        "            extra_fields={\"mlm:artifact_type\": \"torch.save\"},\n",
        "        ),\n",
        "        \"source_code\": pystac.Asset(\n",
        "            title=f\"Source code for {model_name}\",\n",
        "            description=\"GitHub repo of the PyTorch model\",\n",
        "            href=\"https://github.com/Ouranosinc/resoterre\",\n",
        "            media_type=\"text/html\",\n",
        "            roles=[\"mlm:source_code\", \"code\"],\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # --- Additional hyperparameters ---\n",
        "    hyperparameters = {\n",
        "        \"nb_of_new_epochs\": cfg[\"nb_of_new_epochs\"],\n",
        "        \"networks\": cfg[\"networks\"],\n",
        "        \"data_loader\": cfg[\"data_loader\"],\n",
        "        \"optimizers\": cfg[\"optimizers\"],\n",
        "    }\n",
        "\n",
        "    # --- Temporal / spatial extent ---\n",
        "    start_dt = parse_dt(train_cfg[\"start_datetime\"])\n",
        "    end_dt = parse_dt(train_cfg[\"end_datetime\"])\n",
        "    start_dt_str = start_dt.isoformat() + \"Z\"\n",
        "    end_dt_str = end_dt.isoformat() + \"Z\"\n",
        "\n",
        "    # --- Bounding box and geometry ---\n",
        "    coords = cfg[\"north_america_8km_grid_extent_coordinates\"]\n",
        "    bbox = [coords[\"lon_min\"], coords[\"lat_min\"], coords[\"lon_max\"], coords[\"lat_max\"]]\n",
        "    geometry = shapely.geometry.Polygon.from_bounds(*bbox).__geo_interface__\n",
        "\n",
        "    # --- STAC Catalog ---\n",
        "    catalog_href = \"./catalog.json\"  # Catalog file (create if missing)\n",
        "    if not Path(catalog_href).exists():\n",
        "        catalog = pystac.Catalog(\n",
        "            id=\"ml-model-catalog\",\n",
        "            description=\"Catalog for ML model collections\",\n",
        "            title=\"ML Model Catalog\",\n",
        "        )\n",
        "        catalog.set_self_href(catalog_href)\n",
        "        catalog.save()\n",
        "    else:\n",
        "        catalog = pystac.Catalog.from_file(catalog_href)\n",
        "\n",
        "    # --- STAC Collection ---\n",
        "    collection_name = \"ml-model-package\"\n",
        "    collection = pystac.Collection(\n",
        "        id=collection_name,\n",
        "        title=\"Machine Learning Model packaging\",\n",
        "        description=\"Collection of ML model items\",\n",
        "        strategy=pystac.layout.AsIsLayoutStrategy(),\n",
        "        extent=pystac.Extent(\n",
        "            spatial=pystac.SpatialExtent([bbox]),\n",
        "            temporal=pystac.TemporalExtent([[start_dt, end_dt]]),\n",
        "        ),\n",
        "    )\n",
        "    collection_href = \"./ml-model-package/collection.json\"\n",
        "    collection.set_self_href(collection_href)\n",
        "\n",
        "    # --- STAC Item ---\n",
        "    item_name = f\"{model_name.lower()}_rdps_to_hrdps\"\n",
        "    item = pystac.Item(\n",
        "        id=item_name,\n",
        "        collection=collection.id,\n",
        "        geometry=geometry,\n",
        "        bbox=bbox,\n",
        "        datetime=None,\n",
        "        properties={\n",
        "            \"description\": f\"{model_name} trained to downscale RDPS meteorological data to HRDPS resolution.\",\n",
        "            \"start_datetime\": start_dt_str,\n",
        "            \"end_datetime\": end_dt_str,\n",
        "            \"datetime\": None,\n",
        "        },\n",
        "        assets=assets,\n",
        "        extra_fields={\n",
        "            \"mlm:entrypoint\": arch,\n",
        "            \"mlm:hyperparameters\": hyperparameters,\n",
        "        },  # Path to the model class and hyperparameters\n",
        "    )\n",
        "\n",
        "    # --- Apply ML Model Extension ---\n",
        "    item_mlm = MLModelExtension.ext(item, add_if_missing=True)\n",
        "    item_mlm.apply(ml_model_meta.model_dump(by_alias=True, exclude_unset=True, exclude_defaults=True))\n",
        "\n",
        "    # --- Apply Datacube Extension ---\n",
        "    item_dc = DatacubeExtension.ext(item_mlm.item, add_if_missing=True)\n",
        "\n",
        "    # --- Define cube:dimensions ---\n",
        "\n",
        "    levels = [850, 700, 500, 250]  # allowed pressures\n",
        "\n",
        "    dimensions = {\n",
        "        \"time\": Dimension(\n",
        "            properties={\n",
        "                \"type\": \"temporal\",\n",
        "                \"description\": \"Time dimension\",\n",
        "                \"extent\": [start_dt_str, end_dt_str],\n",
        "                \"unit\": \"s\",\n",
        "            }\n",
        "        ),\n",
        "        \"latitude\": Dimension(\n",
        "            properties={\n",
        "                \"type\": \"spatial\",\n",
        "                \"description\": \"Latitude\",\n",
        "                \"extent\": [bbox[1], bbox[3]],\n",
        "                \"axis\": \"y\",\n",
        "                \"unit\": \"degree\",\n",
        "                \"reference_system\": \"EPSG:4326\",\n",
        "            }\n",
        "        ),\n",
        "        \"longitude\": Dimension(\n",
        "            properties={\n",
        "                \"type\": \"spatial\",\n",
        "                \"description\": \"Longitude\",\n",
        "                \"extent\": [bbox[0], bbox[2]],\n",
        "                \"axis\": \"x\",\n",
        "                \"unit\": \"degree\",\n",
        "                \"reference_system\": \"EPSG:4326\",\n",
        "            }\n",
        "        ),\n",
        "        \"level\": Dimension(\n",
        "            properties={\n",
        "                \"type\": \"spatial\",\n",
        "                \"description\": \"Pressure levels\",\n",
        "                \"extent\": [min(levels), max(levels)],\n",
        "                \"values\": levels,\n",
        "                \"axis\": \"z\",\n",
        "                \"unit\": \"hPa\",\n",
        "            }\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    # --- Define cube:variables using rdps_variables and hrdps_variables ---\n",
        "    variables = {}\n",
        "    for var in input_variables + output_variables:\n",
        "        dims = get_variable_dimensions(var)\n",
        "        variables[var] = Variable(\n",
        "            properties={\n",
        "                \"dimensions\": dims,\n",
        "                \"type\": \"data\",\n",
        "                \"data_type\": \"float32\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "    # --- Apply dimensions and variables to Item ---\n",
        "    item_mlm = MLModelExtension.ext(item, add_if_missing=True)\n",
        "    item_mlm.apply(ml_model_meta.model_dump(by_alias=True, exclude_unset=True, exclude_defaults=True))\n",
        "    item_dc = DatacubeExtension.ext(item_mlm.item, add_if_missing=True)\n",
        "    item_dc.apply(dimensions=dimensions, variables=variables)\n",
        "\n",
        "    # --- Now set a fixed flat path ---\n",
        "    item_self_href = Path(\"ml-model-package\") / item_dc.item.id / f\"{item_name}.json\"\n",
        "    item.set_self_href(str(item_self_href))\n",
        "\n",
        "    # --- Add or update catalog, collection and item ---\n",
        "    existing_collection = next((c for c in catalog.get_children() if c.id == collection.id), None)\n",
        "    if not existing_collection:\n",
        "        catalog.add_child(collection)\n",
        "    else:\n",
        "        collection = existing_collection\n",
        "\n",
        "    existing_item = next((i for i in collection.get_all_items() if i.id == item.id), None)\n",
        "    if existing_item:\n",
        "        collection.remove_item(existing_item.id)\n",
        "    collection.add_item(item)\n",
        "\n",
        "    # --- Normalize collection links only ---\n",
        "\n",
        "    # Use '.' as root_href to ensure all relative links are local.\n",
        "    # This avoids absolute paths in the generated STAC JSON\n",
        "    collection.normalize_hrefs(root_href=\".\", strategy=pystac.layout.AsIsLayoutStrategy())\n",
        "\n",
        "    # --- Save collection and item\n",
        "    collection.save_object(include_self_link=True)\n",
        "    item.save_object(include_self_link=True)\n",
        "\n",
        "    # --- Add collection to catalog and save catalog last\n",
        "\n",
        "    # catalog_type=SELF_CONTAINED ensures that all references (collections, items)\n",
        "    # are stored locally within the catalog folder structure respecting relative paths.\n",
        "    # The use of catalog is needed to avoid having absolute paths in the generated STAC JSON file.\n",
        "    catalog.save(catalog_type=pystac.CatalogType.SELF_CONTAINED)\n",
        "\n",
        "    # --- Print relative paths ---\n",
        "    cwd = Path.cwd()\n",
        "    rel_item = Path(item.self_href).relative_to(cwd)\n",
        "    rel_collection = Path(collection.self_href).relative_to(cwd)\n",
        "    print(f\"STAC Item saved to [{rel_item}]\")\n",
        "    print(f\"Collection saved to [{rel_collection}] with {len(list(collection.get_all_items()))} item(s)\")\n",
        "\n",
        "    # --- Fix JSON order ---\n",
        "    top_keys = [\"collection\", \"mlm:entrypoint\", \"mlm:hyperparameters\"]\n",
        "    reorder_stac_json(item.self_href, top_keys)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf8b8daf-9222-4f7e-9557-ac433fbc37ab",
      "metadata": {},
      "source": [
        "## Example Usage "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8949873d-8866-455a-8a74-56c08001983d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "STAC Item saved to [ml-model-package/unet_rdps_to_hrdps/unet_rdps_to_hrdps.json]\n",
            "Collection saved to [ml-model-package/collection.json] with 1 item(s)\n"
          ]
        }
      ],
      "source": [
        "config_folder = \"../configs/downscaling\"\n",
        "aggregated_config = config_aggregator(config_folder)\n",
        "stac_item = create_stac_item_from_config(aggregated_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6646ed4-2216-4b06-a750-a36bd6f69c0c",
      "metadata": {},
      "source": [
        "## Validate the collection and items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "068f48cd-8459-4a43-8455-b161bf8e34ae",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collection loaded successfully\n",
            "Collection ID: ml-model-package\n",
            "Number of items: 1\n",
            "Collection is valid\n",
            "\n",
            "Item loaded successfully\n",
            "Item ID: unet_rdps_to_hrdps\n",
            "Item is valid\n",
            "\n",
            "Assets in item:\n",
            " - model: /path/to/models/unet-model.ckpt\n",
            " - source_code: https://github.com/Ouranosinc/resoterre\n",
            "\n",
            "STAC extensions used in item:\n",
            "['https://stac-extensions.github.io/mlm/v1.5.0/schema.json', 'https://stac-extensions.github.io/datacube/v2.2.0/schema.json']\n"
          ]
        }
      ],
      "source": [
        "collection_path = \"./ml-model-package/collection.json\"\n",
        "item_path = \"./ml-model-package/unet_rdps_to_hrdps/unet_rdps_to_hrdps.json\"\n",
        "\n",
        "# # --- Load Collection ---\n",
        "collection = pystac.Collection.from_file(collection_path)\n",
        "print(\"Collection loaded successfully\")\n",
        "print(f\"Collection ID: {collection.id}\")\n",
        "print(f\"Number of items: {len(list(collection.get_all_items()))}\")\n",
        "\n",
        "# Validate collection\n",
        "try:\n",
        "    collection.validate()\n",
        "    print(\"Collection is valid\")\n",
        "except STACValidationError as e:\n",
        "    print(f\"Collection validation error: {e}\")\n",
        "\n",
        "# --- Load Item ---\n",
        "item = pystac.Item.from_file(item_path)\n",
        "print(\"\\nItem loaded successfully\")\n",
        "print(f\"Item ID: {item.id}\")\n",
        "\n",
        "try:\n",
        "    item.validate()\n",
        "    print(\"Item is valid\")\n",
        "except STACValidationError as e:\n",
        "    print(f\"Item validation error: {e}\")\n",
        "\n",
        "# list assets and extensions\n",
        "print(\"\\nAssets in item:\")\n",
        "for key, asset in item.assets.items():\n",
        "    print(f\" - {key}: {asset.href}\")\n",
        "\n",
        "print(\"\\nSTAC extensions used in item:\")\n",
        "print(item.stac_extensions)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "resoterre",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
